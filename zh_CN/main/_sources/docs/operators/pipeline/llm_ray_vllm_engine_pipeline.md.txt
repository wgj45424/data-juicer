# llm_ray_vllm_engine_pipeline

Pipeline to generate response using vLLM engine on Ray. This pipeline leverages the vLLM engine for efficient large language model inference. More details about ray vLLM engine can be found at: https://docs.ray.io/en/latest/data/working-with-llms.html

ä½¿ç”¨ Ray ä¸Šçš„ vLLM å¼•æ“ç”Ÿæˆå“åº”çš„æµæ°´çº¿ã€‚è¯¥æµæ°´çº¿åˆ©ç”¨ vLLM å¼•æ“å®ç°é«˜æ•ˆçš„å¤§è¯­è¨€æ¨¡å‹æ¨ç†ã€‚æœ‰å…³ Ray vLLM å¼•æ“çš„æ›´å¤šè¯¦æƒ…ï¼Œè¯·å‚è§ï¼šhttps://docs.ray.io/en/latest/data/working-with-llms.html

Type ç®—å­ç±»å‹: **pipeline**

Tags æ ‡ç­¾: gpu

## ğŸ”§ Parameter Configuration å‚æ•°é…ç½®
| name å‚æ•°å | type ç±»å‹ | default é»˜è®¤å€¼ | desc è¯´æ˜ |
|--------|------|--------|------|
| `api_or_hf_model` | <class 'str'> | `'Qwen/Qwen2.5-7B-Instruct'` | API or huggingface model name. |
| `is_hf_model` | <class 'bool'> | `True` |  |
| `system_prompt` | typing.Optional[str] | `None` | System prompt for guiding the optimization task. |
| `accelerator_type` | typing.Optional[str] | `None` | The type of accelerator to use (e.g., "V100", "A100"). Default to None, meaning that only the CPU will be used. |
| `sampling_params` | typing.Optional[typing.Dict] | `None` | Sampling parameters for text generation (e.g., {'temperature': 0.9, 'top_p': 0.95}). |
| `engine_kwargs` | typing.Optional[typing.Dict] | `None` | The kwargs to pass to the vLLM engine. See documentation for details: https://docs.vllm.ai/en/latest/api/vllm/engine/arg_utils/#vllm.engine.arg_utils.AsyncEngineArgs. |
| `api_url` | <class 'str'> | `None` | Base URL of the OpenAI API |
| `api_key` | <class 'str'> | `None` | API key for authentication |
| `kwargs` |  | `''` | Extra keyword arguments. |


## ğŸ”— related links ç›¸å…³é“¾æ¥
- [source code æºä»£ç ](../../../data_juicer/ops/pipeline/llm_inference_with_ray_vllm_pipeline.py)
- [unit test å•å…ƒæµ‹è¯•](../../../tests/ops/pipeline/test_llm_inference_with_ray_vllm_pipeline.py)
- [Return operator list è¿”å›ç®—å­åˆ—è¡¨](../../Operators.md)